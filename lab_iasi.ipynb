{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyAguLOT6Ot0jhZL3rMVm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabitza-tech/iasi_lab_sr/blob/main/lab_iasi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-Shot Speaker Recognition\n",
        "\n",
        "Few-shot learning is a machine learning approach where a model is trained to perform tasks with only a small number of labeled examples. Unlike traditional models that require large amounts of data to learn, few-shot learning enables the model to generalize and make predictions from only a few training samples. This technique is particularly useful in scenarios where data is scarce, and it often involves using pre-trained models, transfer learning, or specialized algorithms designed to leverage minimal data. Few-shot learning is commonly applied in fields like natural language processing, computer vision, and robotics."
      ],
      "metadata": {
        "id": "HBUso5-flXuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting the class based on Similarity between Query and Support\n",
        "\n",
        "The **Cosine Similarity** formula is:\n",
        "\n",
        "$$\n",
        "\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
        "$$\n",
        "\n",
        "The **Euclidean Distance** formula in 2D is:\n",
        "\n",
        "$$\n",
        "d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
        "$$\n",
        "\n",
        "For n-dimensional space, the Euclidean distance is:\n",
        "\n",
        "$$\n",
        "d = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "Iv33wPr7mdTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1\n",
        "\n",
        "In the first part of the lab, we will first try to analyze a couple of samples from two Speaker Recognition databases: VoxCeleb1 and JukeBox Singing.\n",
        "\n",
        "You have inside inside the directory embeddings/ two .pkl format files. Load them using:"
      ],
      "metadata": {
        "id": "VTqKHOoCplB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dataset_file = '/content/jukebox_dev.pkl'\n",
        "merged_dict = np.load(dataset_file,allow_pickle=True)"
      ],
      "metadata": {
        "id": "6JjF_fVEqQVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the .pkl file as a dictionary and we can analyze it."
      ],
      "metadata": {
        "id": "1FVZvuiGqk8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in merged_dict.keys():\n",
        "  print(f'Key: {key}')\n",
        "  print(f'Length of {key} is {len(merged_dict[key])}.')\n",
        "  print(merged_dict[key][:100])\n",
        "  print('---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8VPU85fqvBu",
        "outputId": "7b4e00e8-06ff-46d8-dd29-3d21e889bee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key: concat_labels\n",
            "Length of concat_labels is 3439.\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]\n",
            "---\n",
            "Key: concat_features\n",
            "Length of concat_features is 3439.\n",
            "[[-1.2213382   0.22816841 -1.1387533  ...  1.1900227   0.69100666\n",
            "   0.97755843]\n",
            " [-1.3586781   0.3255885  -1.0633332  ...  1.3752825   0.8211191\n",
            "   0.27719903]\n",
            " [-1.5833865   0.22537938 -1.1752241  ...  1.8129508   0.91189986\n",
            "   0.16760235]\n",
            " ...\n",
            " [ 0.65603006 -0.5826989  -1.0777612  ... -0.13066901  0.270612\n",
            "   0.7298425 ]\n",
            " [ 0.2787906  -0.7626581  -0.53670263 ...  0.36671206  0.50685465\n",
            "   0.45883018]\n",
            " [ 0.1489958  -0.8873958  -0.8141708  ...  0.4536713   0.46052048\n",
            "   0.1998317 ]]\n",
            "---\n",
            "Key: concat_audios\n",
            "Length of concat_audios is 3439.\n",
            "['/media/gabi/gabi_data/jukebox/ALL/1015/11_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_10.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_11.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_12.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_8.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_9.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/12_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_10.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_8.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/13_9.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/14_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/16_8.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_8.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/17_9.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_10.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_11.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_8.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/2_9.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/3_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/6_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_10.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_11.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_12.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_8.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/8_9.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/9_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/0_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/0_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/0_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/0_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/0_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/0_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/0_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_8.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/0_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/1_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/2_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/3_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/4_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/5_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/6_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/6_2.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/6_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/6_4.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/6_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/6_6.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/6_7.wav', '/media/gabi/gabi_data/jukebox/ALL/1046/7_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1066/0_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1066/0_10.wav', '/media/gabi/gabi_data/jukebox/ALL/1066/0_11.wav', '/media/gabi/gabi_data/jukebox/ALL/1066/0_12.wav']\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dictionaries contain three lists:\n",
        "\n",
        "'concat_features': the extracted embeddings from each audio file\n",
        "\n",
        "'concat_labels': the label of each audio file\n",
        "\n",
        "'concat_audios': the filepath of each audio file (we do not need it)"
      ],
      "metadata": {
        "id": "Aqxw7osNrd4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2\n",
        "\n",
        "We will need to separate for each class the samples in a Query set and Support set.\n",
        "\n",
        "NOTE: NO sample should be part of both the Query and Support set!"
      ],
      "metadata": {
        "id": "6hRsL_uTs6cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All unique classes, 56 classes\n",
        "uniq_classes = set(merged_dict['concat_labels'])\n",
        "print(uniq_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_oIrYl7tm2m",
        "outputId": "0ca32a9c-03f7-4c11-df93-1928ac515366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert lists to numpy arrays if needed for indexing\n",
        "concat_labels = np.array(merged_dict['concat_labels'])\n",
        "concat_features = np.array(merged_dict['concat_features'])\n",
        "concat_audios = np.array(merged_dict['concat_audios'])\n",
        "\n",
        "# Initialize empty lists to hold the support and query sets\n",
        "support_labels = []\n",
        "query_labels = []\n",
        "support_features = []\n",
        "query_features = []\n",
        "support_audios = []\n",
        "query_audios = []\n",
        "\n",
        "# Get the unique classes in concat_labels\n",
        "unique_classes = np.unique(concat_labels)\n",
        "\n",
        "# Iterate over each class to split the data\n",
        "for class_label in unique_classes:\n",
        "    # Get the indices of the current class\n",
        "    class_indices = np.where(concat_labels == class_label)[0]\n",
        "\n",
        "    # Split the indices into 80% support and 20% query\n",
        "    train_idx, test_idx = train_test_split(class_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Add the support and query labels/features to the corresponding lists\n",
        "    support_labels.extend(concat_labels[train_idx])\n",
        "    query_labels.extend(concat_labels[test_idx])\n",
        "\n",
        "    support_features.extend(concat_features[train_idx])\n",
        "    query_features.extend(concat_features[test_idx])\n",
        "\n",
        "    support_audios.extend(concat_audios[train_idx])\n",
        "    query_audios.extend(concat_audios[test_idx])\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "support_labels = np.array(support_labels)\n",
        "query_labels = np.array(query_labels)\n",
        "support_features = np.array(support_features)\n",
        "query_features = np.array(query_features)\n",
        "\n",
        "print(query_labels[12:16])\n",
        "print(query_audios[12:16])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBRJ_ejxtvre",
        "outputId": "23cb8732-f3ef-425d-cf29-8995f8e15c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 1]\n",
            "['/media/gabi/gabi_data/jukebox/ALL/1015/16_5.wav', '/media/gabi/gabi_data/jukebox/ALL/1015/11_1.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_3.wav', '/media/gabi/gabi_data/jukebox/ALL/1026/1_5.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g7KLPNVhvOge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few-Shot Concepts**\n",
        "\n",
        "Some important concepts in the Few-Shot literature are:\n",
        "\n",
        "* The number of shots (denoted usually as **K-shots**): the number of samples inside each class from the support.\n",
        "* The number of ways (**n-ways**): the number of different classes inside the Support Set at inference time.\n",
        "* The number of query samples (**n_q**): the number of samples inside the Query Set at inference time.\n",
        "* The number of tasks (**n_tasks**): number of independent inference steps made, for each task, we sample a different support\n",
        "\n",
        "*Inductive Few-Shot*\n",
        "\n",
        "Inductive few-shot implies that each predictions is done on a single sample at a time, and all the other samples inside the Query Set do not influence the results. The most classical approach seen in Machine Learning in general.\n",
        "\n",
        "*Transductive Few-Shot*\n",
        "\n",
        "\n",
        "It's a special case that often appears in practical scenario where multiple query samples are received at inference time and the prediction is made using the whole query set as input, leveraging the statistical distribution of the whole Query set instead of a single sample.\n",
        "\n"
      ],
      "metadata": {
        "id": "if2yH0PswGUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK GENERATOR**\n",
        "\n",
        "We will create a class Task_generator that samples different query and support sets automatically for several tasks, taking as input only the dictionary.\n",
        "\n",
        "**OUR PROBLEM**\n",
        "\n",
        "We want to predict the identity of a person that we have not trained on, based on the Support Set.\n",
        "\n",
        "Each time our Query Set will consist of a single class. (the person we want to identify) We will see how inductive vs transductive methods influence the result."
      ],
      "metadata": {
        "id": "5698Var_zhUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ikoXfIxyzIPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def find_matching_positions(list1, list2):\n",
        "    set_list2 = set(map(tuple, list2))\n",
        "    matching_positions = [i for i, vector in enumerate(list1) if tuple(vector) in set_list2]\n",
        "    return matching_positions\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "class Tasks_Generator:\n",
        "    def __init__(self, uniq_classes, n_tasks=1, n_ways=1251, n_ways_eff=1, n_query=1, k_shot=1, seed=\"42\"):\n",
        "        \"\"\"\n",
        "        uniq_classes: all labels of classes in the support set [List]\n",
        "        n_tasks: number of generated tasks\n",
        "        n_ways: number of sampled classes in the support (number of support classes)\n",
        "        n_ways_eff: number of sampled classes in the query, are part of the support classes (closed set) and much fewer (number of query classes)\n",
        "        n_query: number of samples per query class\n",
        "        k_shot: number of samples per support class\n",
        "        \"\"\"\n",
        "        self.uniq_classes=sorted(uniq_classes)\n",
        "        # Convert all class ids to integer values.\n",
        "\n",
        "        self.n_tasks = n_tasks\n",
        "        self.n_ways= n_ways\n",
        "        self.n_ways_eff = n_ways_eff\n",
        "        self.n_query = n_query\n",
        "        self.k_shot = k_shot\n",
        "        self.seed = seed\n",
        "\n",
        "        set_seed(self.seed)\n",
        "\n",
        "        self.support_classes = []\n",
        "        self.query_classes = []\n",
        "\n",
        "        for i in range(n_tasks):\n",
        "            sampled_classes=sorted(random.sample(self.uniq_classes, n_ways))\n",
        "            self.support_classes.append(sampled_classes)\n",
        "            # Query classes must be part of the sampled support classes\n",
        "            self.query_classes.append(sorted(random.sample(sampled_classes, n_ways_eff)))\n",
        "\n",
        "    def sampler(self, data_dict, mode):\n",
        "        \"\"\"\n",
        "        Every time I sample, I set the seed.. sometimes it doesn't pick the same samples\n",
        "        when I use the sampler second time with a different batch size.\n",
        "\n",
        "        Setting the seed everytime removes this problem\n",
        "\n",
        "        There are 2 modes: query and support. Depending on the mode, we either load the sampled support/query classes for n_tasks\n",
        "        \"\"\"\n",
        "        out_embs = []\n",
        "        out_labels = []\n",
        "        out_slices = []\n",
        "\n",
        "        if mode == \"support\":\n",
        "            tasks_classes = self.support_classes\n",
        "            no_samples = self.k_shot\n",
        "        else:\n",
        "            tasks_classes = self.query_classes\n",
        "            no_samples = self.n_query\n",
        "\n",
        "\n",
        "        for task, sampled_classes in tqdm(enumerate(tasks_classes)):\n",
        "\n",
        "            # Get indices of samples that are part of the sampled classes in the support for this task.\n",
        "            # The query must use the same indices as the support!\n",
        "            self.label_dict = {label:i for i,label in enumerate(self.support_classes[task])}\n",
        "\n",
        "            # Get the indices where elements in concat_labels are in sampled_classes\n",
        "            data_label_indices = np.where(np.isin(np.array(data_dict['concat_labels']), sampled_classes))[0].tolist()\n",
        "\n",
        "            all_labels = np.asarray(data_dict['concat_labels'])[data_label_indices]\n",
        "            all_slices = np.asarray(data_dict['concat_audios'])[data_label_indices]\n",
        "            all_embs = np.asarray(data_dict['concat_features'])[data_label_indices]\n",
        "\n",
        "            combined_array = np.column_stack((all_labels, all_slices))\n",
        "            unique_pairs, inverse_indices = np.unique(combined_array, axis=0, return_inverse=True)\n",
        "\n",
        "            random_pairs = [(label, np.random.choice(unique_pairs[unique_pairs[:, 0] == str(label), 1], size=no_samples, replace=False)) for label in sorted(sampled_classes)]\n",
        "            random_pairs_array = np.concatenate([[[label, id_] for id_ in ids] for label, ids in random_pairs])\n",
        "\n",
        "            data_indices = np.array(find_matching_positions(combined_array, random_pairs_array))\n",
        "\n",
        "            data_embs = all_embs[data_indices]\n",
        "            data_labels = all_labels[data_indices]\n",
        "            data_labels = np.asarray([self.label_dict[label] for label in data_labels])\n",
        "\n",
        "            data_slices = all_slices[data_indices]\n",
        "\n",
        "            out_embs.append(data_embs)\n",
        "            out_labels.append(data_labels)\n",
        "            out_slices.append(data_slices)\n",
        "\n",
        "            #print(\"Sampled classes: \"+str([self.label_dict[label] for label in sampled_classes]))\n",
        "            #print(\"Labels: \" + str(data_labels))\n",
        "\n",
        "        out_embs = np.array(out_embs)\n",
        "        out_labels = np.array(out_labels)\n",
        "        out_slices = np.array(out_slices)\n",
        "\n",
        "        return out_embs, out_labels, out_slices\n",
        "\n",
        "    def sampler_unified(self, data_dict):\n",
        "        \"\"\"\n",
        "        Every time I sample, I set the seed.. sometimes it doesn't pick the same samples\n",
        "        when I use the sampler second time with a different batch size.\n",
        "\n",
        "        Setting the seed everytime removes this problem\n",
        "        \"\"\"\n",
        "        #set_seed(self.seed)\n",
        "        \"\"\"\n",
        "        There are 2 modes: query and support. Depending on the mode, we either load the sampled support/query classes for n_tasks\n",
        "        \"\"\"\n",
        "        support_embs = []\n",
        "        support_labels = []\n",
        "        support_slices = []\n",
        "        support_embs2 = []\n",
        "        support_labels2 = []\n",
        "        support_slices2 = []\n",
        "        query_embs = []\n",
        "        query_labels = []\n",
        "        query_slices = []\n",
        "\n",
        "        only_support_classes = [[x for x in self.support_classes[task] if x not in self.query_classes[task]] for task,sup_classes in enumerate(self.support_classes)]\n",
        "\n",
        "        for task, sampled_classes in tqdm(enumerate(only_support_classes)):\n",
        "            # Get indices of samples that are part of the sampled classes in the support for this task.\n",
        "            # The query must use the same indices as the support!\n",
        "            self.label_dict = {label:i for i,label in enumerate(self.support_classes[task])}\n",
        "            # Get the indices where elements in concat_labels are in sampled_classes\n",
        "            data_label_indices = np.where(np.isin(np.array(data_dict['concat_labels']), sampled_classes))[0].tolist()\n",
        "\n",
        "            all_labels = np.asarray(data_dict['concat_labels'])[data_label_indices]\n",
        "            all_slices = np.asarray(data_dict['concat_audios'])[data_label_indices]\n",
        "            all_embs = np.asarray(data_dict['concat_features'])[data_label_indices]\n",
        "\n",
        "            combined_array = np.column_stack((all_labels, all_slices))\n",
        "            unique_pairs, inverse_indices = np.unique(combined_array, axis=0, return_inverse=True)\n",
        "\n",
        "            random_pairs = [(label, np.random.choice(unique_pairs[unique_pairs[:, 0] == str(label), 1], size=self.k_shot, replace=False)) for label in sorted(sampled_classes)]\n",
        "            random_pairs_array = np.concatenate([[[label, id_] for id_ in ids] for label, ids in random_pairs])\n",
        "\n",
        "            data_indices = np.array(find_matching_positions(combined_array, random_pairs_array))\n",
        "\n",
        "            data_embs = all_embs[data_indices]\n",
        "            data_labels = all_labels[data_indices]\n",
        "            data_labels = np.asarray([self.label_dict[label] for label in data_labels])\n",
        "\n",
        "            data_slices = all_slices[data_indices]\n",
        "\n",
        "            support_embs.append(data_embs)\n",
        "            support_labels.append(data_labels)\n",
        "            support_slices.append(data_slices)\n",
        "\n",
        "        for task, sampled_classes in tqdm(enumerate(self.query_classes)):\n",
        "\n",
        "\n",
        "            # Get indices of samples that are part of the sampled classes in the support for this task.\n",
        "            # The query must use the same indices as the support!\n",
        "            self.label_dict = {label:i for i,label in enumerate(self.support_classes[task])}\n",
        "            # Get the indices where elements in concat_labels are in sampled_classes\n",
        "            data_label_indices = np.where(np.isin(np.array(data_dict['concat_labels']), sampled_classes))[0].tolist()\n",
        "\n",
        "            all_labels = np.asarray(data_dict['concat_labels'])[data_label_indices]\n",
        "            all_slices = np.asarray(data_dict['concat_audios'])[data_label_indices]\n",
        "            all_embs = np.asarray(data_dict['concat_features'])[data_label_indices]\n",
        "\n",
        "            combined_array = np.column_stack((all_labels, all_slices))\n",
        "            unique_pairs, inverse_indices = np.unique(combined_array, axis=0, return_inverse=True)\n",
        "\n",
        "            random_pairs = [(label, np.random.choice(unique_pairs[unique_pairs[:, 0] == str(label), 1], size=(self.k_shot+self.n_query), replace=False)) for label in sorted(sampled_classes)]\n",
        "            random_pairs_array = np.concatenate([[[label, id_] for id_ in ids] for label, ids in random_pairs])\n",
        "\n",
        "            data_indices = np.array(find_matching_positions(combined_array, random_pairs_array))\n",
        "\n",
        "            data_embs = all_embs[data_indices]\n",
        "            data_labels = all_labels[data_indices]\n",
        "            data_labels = np.asarray([self.label_dict[label] for label in data_labels])\n",
        "\n",
        "            data_slices = all_slices[data_indices]\n",
        "\n",
        "            class_s_embs2 =[]\n",
        "            class_s_labels2 =[]\n",
        "            class_s_slices2 =[]\n",
        "            class_q_embs2 =[]\n",
        "            class_q_labels2 =[]\n",
        "            class_q_slices2 =[]\n",
        "            for label in self.query_classes[task]:\n",
        "                label = self.label_dict[label]\n",
        "                indices = np.where(data_labels == label)\n",
        "\n",
        "                class_s_embs2.extend(data_embs[indices][:self.k_shot])\n",
        "                class_s_labels2.extend(data_labels[indices][:self.k_shot])\n",
        "                class_s_slices2.extend(data_slices[indices][:self.k_shot])\n",
        "\n",
        "                class_q_embs2.extend(data_embs[indices][self.k_shot:])\n",
        "                class_q_labels2.extend(data_labels[indices][self.k_shot:])\n",
        "                class_q_slices2.extend(data_slices[indices][self.k_shot:])\n",
        "\n",
        "            support_embs2.append(class_s_embs2)\n",
        "            support_labels2.append(class_s_labels2)\n",
        "            support_slices2.append(class_s_slices2)\n",
        "            query_embs.append(class_q_embs2)\n",
        "            query_labels.append(class_q_labels2)\n",
        "            query_slices.append(class_q_slices2)\n",
        "\n",
        "        support_embs = np.array(support_embs)\n",
        "        support_labels = np.array(support_labels)\n",
        "        support_slices = np.array(support_slices)\n",
        "        support_embs2 = np.array(support_embs2)\n",
        "        support_labels2 = np.array(support_labels2)\n",
        "        support_slices2 = np.array(support_slices2)\n",
        "\n",
        "        support_embs = np.concatenate((support_embs,support_embs2),axis=1)\n",
        "        support_labels = np.concatenate((support_labels,support_labels2),axis=1)\n",
        "        support_slices = np.concatenate((support_slices,support_slices2),axis=1)\n",
        "\n",
        "        query_embs = np.array(query_embs)\n",
        "        query_labels = np.array(query_labels)\n",
        "        query_slices = np.array(query_slices)\n",
        "\n",
        "        return query_embs, query_labels, query_slices,support_embs,support_labels,support_slices"
      ],
      "metadata": {
        "id": "6-aAQbSZvMuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to normalize embeddings\n",
        "\n",
        "def CL2N_embeddings(enroll_embs, test_embs, use_mean=True, use_std=False, eps=1e-10):\n",
        "\n",
        "    all_embs = np.concatenate((enroll_embs,test_embs),axis=1)\n",
        "\n",
        "    initial_shape = all_embs.copy().shape\n",
        "    #if len(initial_shape) == 3:\n",
        "    #    all_embs = all_embs.reshape((-1, all_embs.shape[-1]))\n",
        "\n",
        "    if use_mean:\n",
        "        all_embs = all_embs - np.expand_dims(all_embs.mean(axis=1),1)\n",
        "\n",
        "    if use_std:\n",
        "        all_embs = all_embs / (all_embs.std(axis=1) + eps)\n",
        "\n",
        "    embs_l2_norm = np.expand_dims(np.linalg.norm(all_embs, ord=2, axis=-1), axis=-1)\n",
        "    all_embs = all_embs / embs_l2_norm\n",
        "\n",
        "    #if len(initial_shape) == 3:\n",
        "    #    all_embs = all_embs.reshape(initial_shape)\n",
        "\n",
        "    enroll_embs = all_embs[:,:enroll_embs.shape[1]]\n",
        "    test_embs = all_embs[:,enroll_embs.shape[1]:]\n",
        "\n",
        "    return enroll_embs,test_embs"
      ],
      "metadata": {
        "id": "l63UEJa30-gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB0HCXsezG5v",
        "outputId": "ba0f0986-68ae-4023-fcbf-6e823dfec9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [00:02, 49.47it/s]\n",
            "100it [00:00, 487.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We need to create the similarity functions\n",
        "\n",
        "Now that we have created a set of tasks and splitted each class in a support and query set, we need to define a set of basic similarity functions in order to infer.\n",
        "\n",
        "The first step we have to do, is to compute for each support class a centroid.\n",
        "\n",
        "We will define 3 functions:\n",
        "\n",
        "* Simpleshot (inductive): a simple but very efficient method that utilizes cosine or euclidian distance between each query sample and the centroids of the support set. For each sample, the results may differ.\n",
        "* Simpleshot maj (transductive): we utilize SimpleShot on each sample then do majority voting to obtain the identity of the person\n",
        "* Simpleshot centroid (transductive): we compute the centroid of the query set then compute the similarity metric between the centroid of the query and each support class\n"
      ],
      "metadata": {
        "id": "t6nm71bL1bIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "def majority_or_original(tensor):\n",
        "    majority_labels = []\n",
        "    for task in tensor:\n",
        "        values, counts = task.unique(return_counts=True)\n",
        "        max_count = counts.max().item()\n",
        "        modes = values[counts == max_count]\n",
        "\n",
        "        # If there's a tie (multiple modes), keep the original values for this task\n",
        "        if len(modes) > 1:\n",
        "            majority_labels.append(task)\n",
        "        else:\n",
        "            majority_labels.append(modes.repeat(len(task)))\n",
        "\n",
        "    return torch.stack(majority_labels)\n",
        "\n",
        "class Simpleshot():\n",
        "\n",
        "    def __init__(self,avg=\"mean\",backend=\"cosine\", device='cpu', method=\"inductive\"):\n",
        "        self.avg = avg\n",
        "        self.backend = backend\n",
        "        self.device = torch.device(device)\n",
        "        self.method = method\n",
        "\n",
        "    def eval(self,enroll_embs,enroll_labels,test_embs,test_labels, test_audios):\n",
        "\n",
        "        if self.method == \"ss\":\n",
        "            pred_labels, pred_labels_5 = self.inductive(enroll_embs,enroll_labels,test_embs,test_labels)\n",
        "\n",
        "        elif self.method == \"smv\":\n",
        "            pred_labels, pred_labels_5 = self.inductive(enroll_embs,enroll_labels,test_embs,test_labels)\n",
        "            pred_labels = majority_or_original(pred_labels)\n",
        "\n",
        "        elif self.method == \"sscd\":\n",
        "            pred_labels, pred_labels_5 = self.sscd(enroll_embs,enroll_labels,test_embs,test_labels)\n",
        "\n",
        "        test_labels = torch.from_numpy(test_labels).long()\n",
        "\n",
        "        acc_tasks = compute_acc(pred_labels, test_labels)\n",
        "        acc_tasks_5 = compute_acc_5(pred_labels_5, test_labels)\n",
        "\n",
        "        return acc_tasks, acc_tasks_5, pred_labels_5\n",
        "\n",
        "    def calculate_centroids(self,enroll_embs,enroll_labels):\n",
        "        # Returns [n_tasks,n_ways,192] tensor with the centroids\n",
        "        # sampled_classes: [n_tasks,n_ways]\n",
        "\n",
        "        sampled_classes=[]\n",
        "        for task in enroll_labels:\n",
        "            sampled_classes.append(sorted(list(set(task))))\n",
        "\n",
        "        avg_enroll_embs = []\n",
        "        for i,task_classes in enumerate(sampled_classes):\n",
        "            task_enroll_embs = []\n",
        "\n",
        "            for label in task_classes:\n",
        "                indices = np.where(enroll_labels[i] == label)\n",
        "                if self.avg == \"mean\":\n",
        "                    embedding = (enroll_embs[i][indices[0]].sum(axis=0).squeeze()) / len(indices[0])\n",
        "                if self.avg == \"median\":\n",
        "                    embedding = np.median(enroll_embs[i][indices[0]], axis=0)\n",
        "                task_enroll_embs.append(embedding)\n",
        "            avg_enroll_embs.append(task_enroll_embs)\n",
        "\n",
        "        avg_enroll_embs = np.asarray(avg_enroll_embs)\n",
        "\n",
        "        return avg_enroll_embs\n",
        "\n",
        "    def inductive(self,enroll_embs,enroll_labels,test_embs,test_labels):\n",
        "        \"\"\"\n",
        "        enroll_embs: [n_tasks,k_shot*n_ways,192]\n",
        "        enroll_labels: [n_tasks,k_shot*n_ways]\n",
        "        test_embs: [n_tasks,n_query,192]\n",
        "        test_labels: [n_tasks,n_query]\n",
        "        \"\"\"\n",
        "        # Calculate the mean embeddings for each class in the support\n",
        "        avg_enroll_embs = self.calculate_centroids(enroll_embs, enroll_labels)\n",
        "\n",
        "        test_embs = torch.from_numpy(test_embs).float().to(self.device)\n",
        "        avg_enroll_embs = torch.from_numpy(avg_enroll_embs).float().to(self.device)\n",
        "\n",
        "        if self.backend == \"cosine\":\n",
        "            print(\"Using SimpleShot inductive method with cosine similarity backend\")\n",
        "\n",
        "            avg_enroll_embs = avg_enroll_embs / np.expand_dims(np.linalg.norm(avg_enroll_embs, ord=2, axis=-1),axis=-1)\n",
        "\n",
        "            scores = 1 - torch.einsum('ijk,ilk->ijl', test_embs, avg_enroll_embs)\n",
        "\n",
        "        else:\n",
        "            print(\"Using SimpleShot inductive method with L2 norm backend\")\n",
        "\n",
        "            test_embs = torch.unsqueeze(test_embs,2) # [n_tasks,n_query,1,emb_shape]\n",
        "\n",
        "            avg_enroll_embs = avg_enroll_embs / np.expand_dims(np.linalg.norm(avg_enroll_embs, ord=2, axis=-1),axis=-1)\n",
        "            avg_enroll_embs = torch.unsqueeze(avg_enroll_embs,1)\n",
        "\n",
        "            # Class distance\n",
        "            dist = (test_embs-avg_enroll_embs)**2\n",
        "            scores = torch.sum(dist,dim=-1) # [n_tasks,n_query,1251]\n",
        "\n",
        "        pred_labels = torch.argmin(scores, dim=-1).long()#.tolist()\n",
        "        _,pred_labels_top5 = torch.topk(scores, k=5, dim=-1, largest=False)\n",
        "\n",
        "        return pred_labels, pred_labels_top5\n",
        "\n",
        "\n",
        "    def sscd(self,enroll_embs,enroll_labels,test_embs,test_labels):\n",
        "        \"\"\"\n",
        "        enroll_embs: [n_tasks,k_shot*n_ways,192]\n",
        "        enroll_labels: [n_tasks,k_shot*n_ways]\n",
        "        test_embs: [n_tasks,n_query,192]\n",
        "        test_labels: [n_tasks,n_query]\n",
        "        \"\"\"\n",
        "        # Calculate the mean embeddings for each class in the support\n",
        "\n",
        "        n_query = test_embs.shape[1]\n",
        "        avg_enroll_embs = torch.from_numpy(self.calculate_centroids(enroll_embs, enroll_labels)).float().to(self.device)\n",
        "        avg_test_embs = torch.from_numpy(self.calculate_centroids(test_embs, test_labels)).float().to(self.device)\n",
        "\n",
        "        if self.backend == \"cosine\":\n",
        "            print(\"Using SSCD method with cosine similarity backend.\")\n",
        "\n",
        "            avg_test_embs = avg_test_embs / np.expand_dims(np.linalg.norm(avg_test_embs, ord=2, axis=-1),axis=-1)\n",
        "            avg_enroll_embs = avg_enroll_embs / np.expand_dims(np.linalg.norm(avg_enroll_embs, ord=2, axis=-1),axis=-1)\n",
        "\n",
        "            scores = 1 - torch.einsum('ijk,ilk->ijl', avg_test_embs, avg_enroll_embs).repeat(1,n_query,1)\n",
        "\n",
        "        else:\n",
        "            print(\"Using SSCD method with L2 norm backend.\")\n",
        "            avg_test_embs = avg_test_embs / np.expand_dims(np.linalg.norm(avg_test_embs, ord=2, axis=-1),axis=-1)\n",
        "            avg_enroll_embs = avg_enroll_embs / np.expand_dims(np.linalg.norm(avg_enroll_embs, ord=2, axis=-1),axis=-1)\n",
        "\n",
        "            # Class distance\n",
        "            dist = (avg_test_embs-avg_enroll_embs)**2\n",
        "            dist = torch.unsqueeze(dist,1)\n",
        "            scores = torch.sum(dist,dim=-1).repeat(1,n_query,1) # [n_tasks,n_query,1251]\n",
        "\n",
        "        pred_labels = torch.argmin(scores, dim=-1).long()\n",
        "        _,pred_labels_top5 = torch.topk(scores, k=5, dim=-1, largest=False)\n",
        "\n",
        "        return pred_labels, pred_labels_top5\n",
        "\n",
        "def compute_acc(pred_labels, test_labels):\n",
        "    # Check if the input tensors have the same shape\n",
        "    assert pred_labels.shape == test_labels.shape, \"Shape mismatch between predicted and groundtruth labels\"\n",
        "    # Calculate accuracy for each task\n",
        "    acc_list = (pred_labels == test_labels).float().mean(dim=1).tolist()\n",
        "\n",
        "    return acc_list\n",
        "\n",
        "def compute_acc_5(pred_labels, test_labels):\n",
        "    # Check if the input tensors have the same shape\n",
        "    acc_list = []\n",
        "    for i in range(test_labels.shape[0]):\n",
        "        if test_labels[i][0] in pred_labels[i][0]:\n",
        "            acc_list.append([1])\n",
        "        else:\n",
        "            acc_list.append([0])\n",
        "\n",
        "    acc_list = torch.tensor(np.array(acc_list)).float().mean(dim=1).tolist()\n",
        "\n",
        "    return acc_list"
      ],
      "metadata": {
        "id": "bOvHpA4k1anP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate independent tasks\n",
        "from tqdm import tqdm\n",
        "uniq_classes = sorted(list(set(merged_dict['concat_labels'])))\n",
        "\n",
        "# Parameteres for task creation\n",
        "n_tasks = 100\n",
        "batch_size = 20\n",
        "n_ways = len(uniq_classes)\n",
        "n_ways_eff = 1\n",
        "n_query = 5\n",
        "k_shot = 3\n",
        "seed = 42\n",
        "normalize = True\n",
        "\n",
        "# We create a dictionary where we will save our results\n",
        "acc = {}\n",
        "acc['ss'] = [] # simpleshot results\n",
        "acc['ss_mv'] = [] # majority voting results\n",
        "acc['sscd'] = [] # simpleshot with centroid\n",
        "\n",
        "task_generator = Tasks_Generator(uniq_classes=uniq_classes,\n",
        "                                 n_tasks=n_tasks,\n",
        "                                                n_ways= n_ways,\n",
        "                                                n_ways_eff=n_ways_eff,\n",
        "                                                n_query=n_query,\n",
        "                                                k_shot=k_shot,\n",
        "                                                seed=seed)\n",
        "\n",
        "test_embs, test_labels, test_audios, enroll_embs, enroll_labels, enroll_audios = task_generator.sampler_unified(merged_dict)\n",
        "if normalize:\n",
        "  enroll_embs, test_embs = CL2N_embeddings(enroll_embs,test_embs,use_mean=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1wmdS4D15RF",
        "outputId": "ae3b0ceb-63dd-4b03-aa1f-6418846dce5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [00:02, 36.04it/s]\n",
            "100it [00:00, 556.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for start in tqdm(range(0,n_tasks,batch_size)):\n",
        "  end = (start+batch_size) if (start+batch_size) <= n_tasks else n_tasks\n",
        "  print(start)\n",
        "  x_q,y_q,x_s,y_s = (test_embs[start:end],\n",
        "                  test_labels[start:end],\n",
        "                      enroll_embs[start:end],\n",
        "                      enroll_labels[start:end])\n",
        "\n",
        "  eval = Simpleshot(avg=\"mean\",backend=\"L2\",method=\"ss\")\n",
        "  acc_list, acc_list_5, pred_labels_5 = eval.eval(x_s, y_s, x_q, y_q, test_audios[start:end])\n",
        "  acc[\"ss\"].extend(acc_list)\n",
        "\n",
        "  eval = Simpleshot(avg=\"mean\",backend=\"L2\",method=\"smv\")\n",
        "  acc_list, acc_list_5, pred_labels_5 = eval.eval(x_s, y_s, x_q, y_q, test_audios[start:end])\n",
        "  acc[\"ss_mv\"].extend(acc_list)\n",
        "  eval = Simpleshot(avg=\"mean\",backend=\"L2\",method=\"sscd\")\n",
        "  acc_list, acc_list_5, pred_labels_5 = eval.eval(x_s, y_s, x_q, y_q, test_audios[start:end])\n",
        "  acc['sscd'].extend(acc_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL02HSqu5-fz",
        "outputId": "e9a1ee76-971d-4b48-f271-5b29fac519f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 2/5 [00:00<00:00, 17.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SSCD method with L2 norm backend.\n",
            "20\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SSCD method with L2 norm backend.\n",
            "40\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SSCD method with L2 norm backend.\n",
            "60\n",
            "Using SimpleShot inductive method with L2 norm backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 16.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SSCD method with L2 norm backend.\n",
            "80\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SimpleShot inductive method with L2 norm backend\n",
            "Using SSCD method with L2 norm backend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the results for each method."
      ],
      "metadata": {
        "id": "GM4pBlV66qoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_json = {}\n",
        "final_json['ss'] = 100*sum(acc[\"ss\"])/len(acc[\"ss\"])\n",
        "final_json['ss_mv'] = 100*sum(acc[\"ss_mv\"])/len(acc[\"ss_mv\"])\n",
        "final_json['sscd'] = 100*sum(acc[\"sscd\"])/len(acc[\"sscd\"])\n",
        "\n",
        "for method in final_json.keys():\n",
        "  print(f'Accuracy for method {method} is {final_json[method]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTzjbXQe6k19",
        "outputId": "655646f6-3094-4f9a-ee4f-9db096c9ec61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for method ss is 51.80000093579292\n",
            "Accuracy for method ss_mv is 67.00000005960464\n",
            "Accuracy for method sscd is 75.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdqIgVhQ6w7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}